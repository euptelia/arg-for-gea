""" Make input files for lfmm2 in R package LEA,
using .trees files generated by SliM simulations
tianlin.duan42@gmail.com
2024.04.23
"""
############################# modules #########################################
import msprime
import tskit
import pyslim
import matplotlib.pyplot as plt
import scipy.stats as stats
from scipy.spatial import distance_matrix
import numpy as np
import random
from time import time
import sys # for sys.exit()
import allel # for allel.weir_cockerham_fst()
import os # mkdir

# ############################# options #########################################
import argparse
parser = argparse.ArgumentParser()
parser.add_argument('-i', '--input',
                    help='Input .trees file with its absolute path',
                    type=str)
# parser.add_argument('-o', '--output',
#                     help='Prefix of output files with their absolute path',
#                     type=str)
args = parser.parse_args()

############################# functions #######################################
def LF_fitness(ind_x, ind_y, phenotype,
               optima, dist_mate, sigma_w):
    """Takes x and y coordinates (array-like), phenotypes (array-like),
        environmental optima (array-like), a maximum distance for mating
        (single value), and a standard deviation of fitness function
        (single value).
    Return two arrays of the average fitness of local and foreign individuals,
     respectively.
    v2: time efficient but not memory efficient.
    Try the other version when the number of individuals is large.
    """
    optima=np.array(optima)
    coord = list(zip(ind_x, ind_y))
    dist_matrix = distance_matrix(coord, coord)
    isLocal_matrix = dist_matrix <= dist_mate
    # Calculate fitness matrix (w_matrix[i][j]: individual j at location i) with broadcast
    w_matrix = 1.0 + stats.norm.pdf(np.array(phenotype),
                                    np.array(optima)[:, np.newaxis],
                                    sigma_w)
    # relative fitness
    w_matrix = w_matrix / np.mean(w_matrix, 1)[:, np.newaxis]
    #Average relative fitness of local and foreign individuals
    w_local = np.mean(np.ma.masked_array(w_matrix, np.invert(isLocal_matrix)), 1)
    w_foreign = np.mean(np.ma.masked_array(w_matrix, isLocal_matrix), 1)
    return w_local, w_foreign

############################# program #########################################
#User input arguments:
path_file_name = args.input
# outPath = args.output
# Testing
# Values
sigma_w = 0.4
# sigma_w = 1.0
# sigma_w = 2.0
dist_mate = 0.15
sample_size = 500
# path_file_name = ("/home/tianlin/Documents/github/data/slim_data"
#                   "/glacial_history/M2b_highPoly_highMig_patchyMap/batch1"
#                   "/Continuous_nonWF_M2b_glacialHistory_patchyMap_mu1.0e"
#                   "-08_sigmaM0.01_sigmaW0.4_sigmaD0.06_mateD0"
#                   ".15_seed15101314668201809_tick110000.trees")
# path_file_name = ("/home/tianlin/Documents/github/data/slim_data/"
#                   "glacial_history/historical_optimum_0/"
#                   "M2b_lowPoly_highMig_clineMap/batch1/"
#                   "Continuous_nonWF_M2b_glacialHistoryOptimum0_clineMap_"
#                   "mu1.0e-09_sigmaM0.1_sigmaW0.4_sigmaD0.06_mateD0.15_"
#                   "seed101863569341019597_tick110000.trees")
# path_file_name = ("/home/tianlin/Documents/github/data/slim_data/glacial_history/historical_optimum_0/M2b_highPoly_lowMig_clineMap/batch1/Continuous_nonWF_M2b_glacialHistoryOptimum0_clineMap_mu1.0e-08_sigmaM0.01_sigmaW0.4_sigmaD0.03_mateD0.12_seed166332697017507196_tick110000.trees")
# path_file_name = "/home/tianlin/Documents/github/data/slim_data/glacial_history/historical_optimum_0/M2b_highPoly_highMig_clineMap/batch1/Continuous_nonWF_M2b_glacialHistoryOptimum0_clineMap_mu1.0e-08_sigmaM0.01_sigmaW0.4_sigmaD0.06_mateD0.15_seed147836278104916407_tick110000.trees"
# path_file_name = "/home/tianlin/Documents/github/data/slim_data/glacial_history/historical_optimum_0/M2b_lowPoly_lowMig_clineMap/batch1/Continuous_nonWF_M2b_glacialHistoryOptimum0_clineMap_mu1.0e-09_sigmaM0.1_sigmaW0.4_sigmaD0.03_mateD0.12_seed110027493345619720_tick110000.trees"


outPath = "/home/tianlin/Documents/github/data/R/20240507/"

file_name = path_file_name.split("/")[-1]
model_name = file_name[0:-6]  # Assuming the file name extension is .trees
# Delete seed and tick information for making a directory
short_model_name = "_".join(file_name.split("_")[0:-2])
if not os.path.exists(outPath):
    os.makedirs(outPath)

# Tree-sequence file from SLiM
ts = tskit.load(path_file_name)
current_tick = ts.metadata['SLiM']['tick']
ind_x, ind_y, ind_z = zip(*ts.individuals_location)
# Genomic position of sites
pos = ts.sites_position
# Genomic position of mutations
pos_by_mut = ts.sites_position[ts.mutations_site]
# Maximum tick
max_tick = ts.metadata["SLiM"]["tick"]
# Number of diploid individuals
N = ts.num_individuals
# Environmental optima
optima = np.array(ts.metadata['SLiM']['user_metadata']['indsOptimum'])
mapValues = np.array(ts.metadata['SLiM']['user_metadata']['mapValues'])

# Recapitation: better skipped as no functional mutation was added during this stage
# ts_recap = pyslim.recapitate(ts, ancestral_Ne=5e3,
#                              recombination_rate=1e-7,
#                              random_seed=1)
ts_recap = ts
# Add neutral mutations
mutation_seed = random.randint(1, 2**31)
# mutation_seed = 1707743957
mut_model1 = msprime.SLiMMutationModel(type=1)
mts = msprime.sim_mutations(ts_recap,
                            rate=5e-8,
                            random_seed=mutation_seed,
                            model=mut_model1,
                            keep=True) #keep the existing mutations

# Phenotypic effect of each mutation
# 1-dimensional
mut_effect = []
for mut in mts.mutations():
    mut_effect.append(mut.metadata['mutation_list'][0]['selection_coeff'])
mut_effect = np.array(mut_effect)

# 2-dimensional: list of lists, corresponding to phenotypic effect of allele 0/1/2... at each site
mut_effect_lists = []
for site in mts.sites():
    effect_site = [0]
    for mut in site.mutations:
        effect_site.append(mut.metadata['mutation_list'][0]['selection_coeff'])
    mut_effect_lists.append(effect_site)

# Observed extent of local adaptation (local-foreign contrast (LF))
(w_local, w_foreign) = LF_fitness(ind_x, ind_y, ind_z,
                                  optima, dist_mate, sigma_w)
LF_cline = w_local - w_foreign
mean_LF = np.mean(LF_cline)


# # Method 1: Not used
# # Write genotype matrix in vcf format:
# # met an error while reading with vcf2geno (LEA)
# fout = open(outPath + model_name + ".vcf", "w")
# mts.write_vcf(fout)
# fout.close()

# Method 2: Take 40% memory while running
# Directly write .geno files, discarding all multi-allelic sites
# and sites that have been fixed/lost
#Index of biallelic sites


# 2.1: Remove sites has/had more than two alleles
isBiallelic1 = np.full(shape=mts.num_sites, fill_value=True)
i = 0
for s in mts.sites():
    if len(s.alleles) > 2:
        isBiallelic1[i] = False
    i += 1

#2.2 Remove fixed/lost sites, keep alleles that are currently biallelic
isBiallelic2 = np.full(shape=mts.num_sites, fill_value=True)
i = 0
for v in mts.variants():
    if len(set(v.genotypes)) != 2:
        isBiallelic2[i] = False
    i += 1
# Sites that have never had more than 2 alleles and currently segregating:
isBiallelic = isBiallelic1 & isBiallelic2

# Took a lot of memeory: Needs to be rewrite by loops
bialle_geno_mat = mts.genotype_matrix()[isBiallelic]
# Add up the two genomes of each diploid individual
diploid_geno_mat = bialle_geno_mat.reshape(bialle_geno_mat.shape[0],
                                           int(bialle_geno_mat.shape[1]/2),
                                           2).sum(axis=2)
del bialle_geno_mat
# Generate a 500-individual sample
sample_ids = sorted(np.random.choice(ts.num_individuals,
                                            size=sample_size,
                                            replace=False))
sample_mat = diploid_geno_mat[:, sample_ids]
# Remove sites that not vary among sampled individuals
allele_sum = np.sum(sample_mat, axis=1)
sample_mat = sample_mat[np.logical_and(allele_sum != 0,
                                       allele_sum != sample_size*2), :]
foutGeno = open(outPath + model_name +
                "_mutSeed" + str(mutation_seed) +
                "_500Inds.geno", "w")
# for i in range(diploid_geno_mat.shape[0]):
#     words = [str(gt) for gt in list(diploid_geno_mat[i])]
#     foutGeno.write("".join(words) + "\n")
for i in range(sample_mat.shape[0]):
    words = [str(gt) for gt in list(sample_mat[i])]
    foutGeno.write("".join(words) + "\n")
foutGeno.close()

# Write the corresponding .env file
# "This format corresponds to a matrix in which each variable is

# represented as a column and each sample is represented as a row (LEA)"
sample_optima = optima[sample_ids]
foutEnv = open(outPath + model_name +
               "_mutSeed" + str(mutation_seed) +
               "_500Inds.env", "w")
for i in range(len(sample_optima)):
    line = str(sample_optima[i]) + "\n"
    foutEnv.write(line)
foutEnv.close()


# Observed extent of local adaptation (local-foreign contrast (LF))
(w_local, w_foreign) = LF_fitness(ind_x, ind_y, ind_z,
                                  optima, dist_mate, sigma_w)
LF_cline = w_local - w_foreign
mean_LF = np.mean(LF_cline)
# Overall extent of local adaptation
#print(mean_LF)


# Contribution of each mutation to LF
# Version 3: LF_site
# randomizing the distribution of the mutation
# Skip calculation for neutral mutations and add 0
# Skip multi-allelic sites and fixed/lost sites
shuffle_replicates = 1
LF_shuffle = []
t = time()
index_site = 0
# traverse sites with mutations
for v in mts.variants():
    if isBiallelic[index_site]:
        # Skip calculation and directly append mean_LF as LF_shuffle
        # By definition neutral mutations should not affect phenotypes at all
        if sum(mut_effect_lists[index_site]) == 0:
            index_gt = 1
            while index_gt < len(mut_effect_lists[index_site]):
                LF_shuffle.append(mean_LF)
                index_gt += 1
        else:
            # Calculate the effect of each mutation at the focal site
            index_gt = 1
            # traverse all derived alleles at the site
            while index_gt < len(mut_effect_lists[index_site]):
                # for multiallelic sites, hide other alleles at the site except the focal allele,
                # leave only one allele at each time
                focal_gt = np.array(v.genotypes)
                focal_gt[focal_gt != index_gt] = 0
                # effect of the focal mutation in each individual
                effect_mut_genomes = np.array(mut_effect_lists[index_site])[focal_gt]
                # Add up the two genomes of each individual
                effect_mut_ind = (effect_mut_genomes[range(0, 2 * N - 1, 2)] +
                                  effect_mut_genomes[range(1, 2 * N, 2)])
                phenotype_without_mut = ind_z - effect_mut_ind
                # Shuffle the distribution of the individuals
                # (without changing the observed heterozygosity)
                r = 0
                LF_shuffle_mut = np.zeros(shuffle_replicates)
                while r < shuffle_replicates:
                    effect_mut_ind_shuffle = list(effect_mut_ind)  # shallow copy
                    np.random.shuffle(effect_mut_ind_shuffle)
                # print(list(effect_mut_ind))
                # print(effect_mut_ind_shuffle)
                # Phenotype with shuffled mut =
                # phenotypes of each ind - effect of the focal mutation in each ind
                # + effect of the focal mutation in each ind after shuffling
                    phenotype_shuffle = phenotype_without_mut + effect_mut_ind_shuffle
                    (w_local, w_foreign) = LF_fitness(ind_x, ind_y, phenotype_shuffle,
                                                      optima, dist_mate, sigma_w)
                    LF_shuffle_mut[r] = np.mean(w_local - w_foreign)
                    r += 1
                LF_shuffle.append(np.mean(LF_shuffle_mut))
                index_gt += 1
    index_site += 1
    if index_site % 10000 == 0:
        print(f"{index_site} sites processed")
delta_LF_site = mean_LF-LF_shuffle
# Elapsed Time
timer = time() - t
print(timer/60)

#Delete the sites that are not polymorphic in samples
delta_LF_site = delta_LF_site[np.logical_and(allele_sum != 0,
                                       allele_sum != sample_size*2)]
foutLf = open(outPath + model_name +
               "_mutSeed" + str(mutation_seed) +
               "_LFsite_500Inds.txt", "w")
for i in range(len(delta_LF_site)):
    line = str(delta_LF_site[i]) + "\n"
    foutLf.write(line)
foutLf.close()
